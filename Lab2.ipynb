{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Lab 2: Exploratory Data Analysis (EDA) with Synthetic ABCD Opioid Use Data\n",
    "**Learning Objectives**\n",
    "\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "1. **Summarize numeric variables** using key descriptive statistics (median, IQR, percent missing) and visualizations (histograms, boxplots).\n",
    "2. **Summarize categorical variables** using frequency tables, proportions, and bar plots.\n",
    "3. **Explore relationships between variables** by:\n",
    "   - Comparing two categorical variables\n",
    "   - Comparing a categorical variable with a numeric variable\n",
    "   - Comparing two numeric variables\n",
    "4. **Apply helper functions** to:\n",
    "   - Standardize and handle special missing codes\n",
    "   - Generate consistent summaries for numeric and categorical variables\n",
    "5. **Interpret EDA results** in plain language, connecting statistical patterns to the research context.\n",
    "\n",
    "> Note: All work in this lab is **exploratory**. You will not clean or transform data until **Lab 3**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "**Grading Note:**  \n",
    "Complete all sections of this notebook, then **save it as `Lab2.ipynb`** before submission.  \n",
    "If you are working in VS Code/Codespaces, use **File ‚Üí Download** to save the `.ipynb` file to your local machine.  \n",
    "Refer to the **Autograder Checklist & Grade Rubric** at the end of this notebook to make sure your work meets all requirements.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## About the Sample & Dataset\n",
    "\n",
    "This lab uses a **synthetic ABCD-like dataset** of **young adults age ~20 (N ‚âà 11,680)**. It combines self-report survey measures with a limited set of hair toxicology results to practice EDA and, later, data cleaning.\n",
    "\n",
    "**What‚Äôs inside (high level):**\n",
    "- **Opioid use history & patterns:** lifetime and past-year nonmedical use; ages of onset/regular use; days since last use; lifetime pill counts; largest single-sitting amount; specific products (e.g., hydrocodone, oxycodone, codeine, morphine) and **routes** (oral, intranasal, smoking/vaping).\n",
    "- **Problems & risk:** **DAPI** summary score (0‚Äì72), **parental monitoring** mean, **peer deviance** items (PGD 001‚Äì008), perceived harm, peer approval, and opportunity to use.\n",
    "- **Hair toxicology (subset ~20%)**: opiate **screen** (positive/negative/QNS) plus **confirm** and **quantitation** targets for several analytes.\n",
    "\n",
    "#### **Variable Look-Up:** Remember to look up variables in the ABCD Data Dictionary for data structures and coding schemes: https://abcd.deapscience.com/#/my-datasets/create-dataset\n",
    "\n",
    "**Coding & missingness to expect:**\n",
    "- Binary variables typically use **0 = No, 1 = Yes**; ordinal items follow documented scales (e.g., 0‚Äì4).\n",
    "- **Special nonresponse codes** appear in some fields (e.g., **666 = QNS**, **777 = Refused**, **999 = Don‚Äôt know**).\n",
    "- **NaN** often indicates **gating/skip logic** (question not asked) or non-tested toxicology rows.\n",
    "\n",
    "> We will treat special codes transparently during EDA and address recoding/cleaning decisions in the next lab.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "\n",
    "## Setup\n",
    "\n",
    "Run the following cell to import the necessary libraries and load the dataset.\n",
    "\n",
    "### What this setup code does\n",
    "\n",
    "- Imports three libraries:\n",
    "  - pandas (pd) for working with tables of data\n",
    "  - numpy (np) for numeric operations\n",
    "  - matplotlib.pyplot (plt) for plotting\n",
    "- A pandas DataFrame is a table: rows are observations (people/records) and columns are variables (features). `df` is the DataFrame loaded from the CSV file.\n",
    "- `pd.set_option('display.max_columns', None)` tells pandas to show all columns when printing tables.\n",
    "- `pd.read_csv('L2L3dataset.csv')` reads the dataset into `df`.\n",
    "- `df.shape` shows how many rows and columns are in the data, and we print that.\n",
    "- `df.head()` displays the first 5 rows so you can quickly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df = pd.read_csv('L2L3dataset.csv')\n",
    "\n",
    "# Display the shape of the dataset and the first few rows\n",
    "df_shape = df.shape\n",
    "print(f\"Dataset contains {df_shape[0]} rows and {df_shape[1]} columns.\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "### (Optional but Recommended) Explore the Dataset with Data Wrangler\n",
    "\n",
    "Data Wrangler offers a user-friendly interface for viewing and cleaning your data. Here‚Äôs how to launch and use it directly within your notebook:\n",
    "\n",
    "1. **Open Data Wrangler from the Notebook:**  \n",
    "   - Ensure that your dataset is loaded as a Pandas DataFrame (e.g., `df`).\n",
    "   - In the Jupyter > Variables panel (usually located in the right sidebar), locate the variable `df`.\n",
    "   - Click the **Open 'df' in Data Wrangler** button next to the variable name.\n",
    "\n",
    "2. **Explore Data in Viewing Mode:**  \n",
    "   - Data Wrangler will open in Viewing Mode by default.\n",
    "   - Use this mode to quickly inspect your data, view summary statistics, and identify any obvious issues like missing values or outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Figure dir + robust savefig helper (works with or without extension or path)\n",
    "IMG_DIR = Path(\"figures\")\n",
    "IMG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def savefig(path, dpi=150, tight=True):\n",
    "    \"\"\"\n",
    "    Save current Matplotlib figure.\n",
    "    Examples:\n",
    "      savefig(\"03_demo_cat_bar\")\n",
    "      savefig(\"figures/03_demo_cat_bar.png\")\n",
    "    \"\"\"\n",
    "    p = Path(path)\n",
    "    if p.suffix == \"\":\n",
    "        p = p.with_suffix(\".png\")\n",
    "    if p.parent == Path(\".\"):\n",
    "        p = IMG_DIR / p  # default to ./figures/\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(p, dpi=dpi, bbox_inches=\"tight\" if tight else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## 1. Univariate EDA: Numeric variable (Demo)\n",
    "\n",
    "We'll start by examining a quantitative variable.  In this example we use the **DAPI sum** (`su_y_drgprob_prsum`).  The code below calculates the median, interquartile range (IQR), and percent missing.  It then visualizes the distribution with a histogram and a boxplot.  When you run the cell, look at the histogram shape and the boxplot for signs of skewness or outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the numeric variable\n",
    "dapi_var = 'su_y_drgprob_prsum'\n",
    "dapi = df[dapi_var]\n",
    "\n",
    "# Summary statistics\n",
    "mean = dapi.mean()\n",
    "median = dapi.median()\n",
    "q1 = dapi.quantile(0.25)\n",
    "q3 = dapi.quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "pct_missing = dapi.isna().mean() * 100\n",
    "min_val = dapi.min()\n",
    "max_val = dapi.max()\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Median: {median}\")\n",
    "print(f\"Q1 (25th percentile): {q1}\")\n",
    "print(f\"Q3 (75th percentile): {q3}\")\n",
    "print(f\"IQR (Q3 - Q1): {iqr}\")\n",
    "print(f\"Percent missing: {pct_missing:.2f}%\")\n",
    "print(f\"Min: {min_val}\")\n",
    "print(f\"Max: {max_val}\")\n",
    "\n",
    "# Histogram\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(dapi.dropna(), bins=20, edgecolor='black')\n",
    "plt.title('Distribution of DAPI sum')\n",
    "plt.xlabel(dapi_var)\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "savefig(\"q1_dapi_hist\")\n",
    "plt.show()\n",
    "\n",
    "# Boxplot\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.boxplot(dapi.dropna(), vert=False)\n",
    "plt.title('Boxplot of DAPI sum')\n",
    "plt.xlabel(dapi_var)\n",
    "plt.tight_layout()\n",
    "savefig(\"q1_dapi_box\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "\n",
    "**Reflection:**\n",
    "\n",
    "- Describe the shape of the DAPI sum distribution.  Does it look symmetric, skewed, or something else?\n",
    "- Are there any obvious outliers?  How might those affect later analyses?\n",
    "- What percent of cases have missing values for this variable?\n",
    "\n",
    "Use the bullet points above to jot down your observations in the cell below.\n",
    "\n",
    "- The DAPI sum distribution is right-skewed\n",
    "- The boxplot shows many high outliers, which match the long right tail seen in the histogram.\n",
    "- Yes, there are obvious outliers, and they could pull the mean upward and make the data look more variable than it really is.\n",
    "- These outliers could also affect statistical tests that assume normality or make some models less accurate unless the data is transformed or the outliers are examined.\n",
    "- 0.47%\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## 2. Your turn: Univariate EDA ‚Äî Quantitative\n",
    "\n",
    "Pick one approved numeric variable from the list below and compute:\n",
    "\n",
    "- **Median**\n",
    "- **IQR (Q3 ‚àí Q1)**\n",
    "- **Percent missing**\n",
    "\n",
    "Then make a histogram and a boxplot.  \n",
    "Treat special codes `777` and `999` as missing by replacing them with `np.nan` first.\n",
    "\n",
    "### Approved quantitative variables\n",
    "- `su_y_sui__rxopi__onset_useage` ‚Äì Age at first opioid use\n",
    "- `su_y_sui__rxopi__lt_001` ‚Äì Number of opioid use days (past 6 months)\n",
    "- `fc_y_pm_mean` ‚Äì Parental monitoring mean\n",
    "\n",
    "### Copilot prompts for further understanding (paste one at a time):\n",
    "1. ‚ÄúShow how to compute median, IQR (Q3‚àíQ1), and percent missing for a pandas Series; treat 777 and 999 as NaN without changing the original DataFrame.‚Äù\n",
    "2. ‚ÄúGiven a numeric pandas Series, provide minimal code to plot a histogram and a horizontal boxplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select your numeric variable\n",
    "quant_var = 'su_y_sui__rxopi__lt_001'  # TODO: choose from the approved list above\n",
    "series = df[quant_var].replace({777: np.nan, 999: np.nan})  # Replace special missing codes with NaN\n",
    "\n",
    "# Summary statistics\n",
    "# Summary statistics\n",
    "mean = series.mean() # TODO\n",
    "median = series.median() # TODO\n",
    "q1 = series.quantile(0.25) # TODO (0.25)\n",
    "q3 = series.quantile(0.75) # TODO\n",
    "iqr = q3 - q1\n",
    "pct_missing = series.isna().mean() * 100 # TODO\n",
    "min_val = series.min() # TODO\n",
    "max_val = series.max() # TODO\n",
    "\n",
    "print(f\"Variable: {quant_var}\")\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Median: {median}\")\n",
    "print(f\"Q1 (25th percentile): {q1}\")\n",
    "print(f\"Q3 (75th percentile): {q3}\")\n",
    "print(f\"IQR (Q3 - Q1): {iqr}\")\n",
    "print(f\"Percent missing: {pct_missing:.2f}%\")\n",
    "print(f\"Min: {min_val}\")\n",
    "print(f\"Max: {max_val}\")\n",
    "\n",
    "# Histogram\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(series.dropna(), bins=20, edgecolor='black')\n",
    "plt.title(f'Distribution of {quant_var}')\n",
    "plt.xlabel(quant_var)\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"hist_{quant_var}.png\", dpi=150, bbox_inches=\"tight\")\n",
    "savefig(f\"hist_{quant_var}\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "# Boxplot\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.boxplot(series.dropna(), vert=False)\n",
    "plt.title(f'Boxplot of {quant_var}')\n",
    "plt.xlabel(quant_var)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"box_{quant_var}.png\", dpi=150, bbox_inches=\"tight\")\n",
    "savefig(f\"box_{quant_var}\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## 3. Univariate EDA: Categorical variable (Demo)\n",
    "\n",
    "Next we'll explore a categorical variable.  We'll use **self‚Äëreported opioid use** over the past 6¬†months (`su_y_sui__use__rxopi_001`), which takes on values 1 (yes) or 0 (no).  The code below creates a frequency table with counts and proportions and then draws a bar chart.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select the categorical variable\n",
    "SR_var = 'su_y_sui__use__rxopi_001'\n",
    "cat_series = df[SR_var]\n",
    "\n",
    "# Frequency counts and proportions\n",
    "counts = cat_series.value_counts(dropna=False)\n",
    "proportions = cat_series.value_counts(normalize=True, dropna=False)\n",
    "\n",
    "freq_table = pd.DataFrame({\n",
    "    'count': counts,\n",
    "    'proportion': proportions\n",
    "})\n",
    "print(freq_table)\n",
    "\n",
    "# Bar plot\n",
    "plt.figure(figsize=(5,4))\n",
    "counts.plot.bar()\n",
    "plt.title('Self‚Äëreported opioid use (past 6¬†months)')\n",
    "plt.xlabel('Response (1=yes, 0=no, NaN=missing)')\n",
    "plt.ylabel('Count')\n",
    "savefig(\"q3_sr_bar\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "\n",
    "**Reflection (Ungraded, for Note-Taking)**\n",
    "\n",
    "- What proportion of participants report opioid use in the past 6¬†months?  What proportion report no use?  How much missing data is there?\n",
    "- Based on the bar chart, does this variable look balanced or imbalanced?  How might that influence analyses that use it as a grouping variable?\n",
    "\n",
    "\n",
    "/\n",
    "\n",
    "Proportion reporting opioid use (yes = 1): about 10.3% of participants.\n",
    "\n",
    "Proportion reporting no use (0): about 89.1% of participants.\n",
    "\n",
    "Missing data: about 0.6% of cases are missing.\n",
    "\n",
    "/\n",
    "\n",
    "The variable is imbalanced, with a large majority reporting no opioid use and only a small minority reporting use.\n",
    "\n",
    "May affect analyses because the ‚Äúyes‚Äù group is much smaller, which can reduce statistical power, make group comparisons uneven, and potentially lead to unstable model estimates if used as a predictor or grouping variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 4. Your turn: Univariate EDA ‚Äî Categorical\n",
    "\n",
    "Pick one approved categorical variable from the list below and:\n",
    "\n",
    "- Create a **frequency table** with counts and proportions.\n",
    "- Make a **bar plot** of the counts.\n",
    "\n",
    "Treat special codes `999` as missing by replacing them with `np.nan` first (if applicable).\n",
    "\n",
    "### Approved categorical variables\n",
    "- `su_y_sui__use__rxopi_001` ‚Äì Ever used opioids\n",
    "- `su_y_perc__rxopi_001` ‚Äì Percieved harm of use\n",
    "- `su_y_otu__rxopi_001` ‚Äì Had opportunity to use opioid (1=yes, 0=no)\n",
    "- `su_y_ptu__rxopi_001` ‚Äì Friend acceptance of opioid use (0, 1, 2; 999=missing)\n",
    "\n",
    "### Copilot prompts for further understanding (paste one at a time):\n",
    "1. ‚ÄúIn pandas, how do I get counts and proportions for each category in a Series, including NaN values?‚Äù\n",
    "2. ‚ÄúHow do I replace special numeric codes (e.g., 999) with NaN in a pandas Series without altering the original DataFrame?‚Äù\n",
    "3. ‚ÄúWrite minimal matplotlib code to make a bar chart from a pandas value_counts() result and label the axes.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select your categorical variable\n",
    "qual_var = 'su_y_ptu__rxopi_001'  # TODO: choose from the approved list above\n",
    "series = df[qual_var].replace({999: np.nan})  # TODO: fill special missing code(s) if applicable\n",
    "\n",
    "# Frequency counts and proportions\n",
    "counts = series.value_counts(dropna=False)             # TODO: counts\n",
    "proportions = series.value_counts(normalize=True, dropna=False)  # TODO: proportions\n",
    "\n",
    "# Combine into a DataFrame\n",
    "freq_table = pd.DataFrame({\n",
    "    'count': counts,\n",
    "    'proportion': proportions\n",
    "})\n",
    "print(freq_table)\n",
    "\n",
    "# Bar plot\n",
    "plt.figure(figsize=(5,4))\n",
    "counts.plot.bar()\n",
    "plt.title(f'Distribution of {qual_var}')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"bar_{qual_var}.png\", dpi=150, bbox_inches=\"tight\")\n",
    "savefig(f\"bar_{qual_var}\")\n",
    "plt.show()     \n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "\n",
    "**Reflection:**\n",
    "\n",
    "Write a brief interpretation of the distributions you plotted above.\n",
    "\n",
    "- Does your numeric variable appear symmetric, skewed, or bimodal?  Are there outliers?\n",
    "- Are the categories of your categorical variable balanced or imbalanced?  Is there a large amount of missing data?\n",
    "\n",
    "Add your notes here.\n",
    "\n",
    "- right-skewed, yes there are outliers\n",
    "- imbalanced. Category 2 is the largest (about 60%), category 1 is smaller (about 19%), and category 0 is the smallest (about 14%).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## 5. Bivariate EDA: Categorical √ó Categorical (Demo)\n",
    "\n",
    "We'll now explore the relationship between two categorical variables.  We'll cross **self‚Äëreported opioid use** in the past 6¬†months (`su_y_sui__use__rxopi_001`) with the **hair toxicology screen** result (`su_y_hairtox__rslt__opi_scrn`).  Our goal is to see how often the self‚Äëreport and the hair toxicology agree or disagree.\n",
    "\n",
    "The code below creates a contingency table of counts and proportions.  It also calculates the **discordant percent**‚Äîthe percentage of cases where the self‚Äëreport is negative but the hair toxicology is positive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define variables\n",
    "self_report = 'su_y_sui__use__rxopi_001__l'\n",
    "hair_screen = 'su_y_hairtox__rslt__opi_scrn'\n",
    "\n",
    "# Select the two variables\n",
    "x = df[self_report]\n",
    "y = df[hair_screen]\n",
    "\n",
    "# Create contingency table (2√ó2) with NaN treated as a category\n",
    "contingency = pd.crosstab(x.fillna('Missing'), y.fillna('Missing'))\n",
    "print('Contingency table (counts):')\n",
    "print(contingency)\n",
    "\n",
    "# Row and column proportions\n",
    "row_props = contingency.div(contingency.sum(axis=1), axis=0)\n",
    "col_props = contingency.div(contingency.sum(axis=0), axis=1)\n",
    "\n",
    "print('Row proportions:')\n",
    "print(row_props)\n",
    "print('Column proportions:')\n",
    "print(col_props)\n",
    "\n",
    "# Calculate discordant percentage: self‚Äëreport = 0 (no use) and hair tox = 1 (positive)\n",
    "discordant_mask = (x == 0) & (y == 1)\n",
    "num_discordant = discordant_mask.sum()\n",
    "total_nonmissing = ((~x.isna()) & (~y.isna())).sum()\n",
    "\n",
    "discordant_percent = (num_discordant / total_nonmissing) * 100 if total_nonmissing > 0 else np.nan\n",
    "\n",
    "print(f\"Percent of cases with self‚Äëreport = 0 and hair tox = 1: {discordant_percent:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "**Tip:**\n",
    "\n",
    "- Ask copilot to help you organize the cross-tabs into a clearer format with plainer language, this will help you read and understand the output.\n",
    "\n",
    "**Reflection:**\n",
    "\n",
    "- How often do the self‚Äëreport and the hair toxicology agree?  Where are the disagreements?\n",
    "- What does the discordant percentage tell you about under‚Äëreporting or over‚Äëreporting of opioid use in this sample?\n",
    "\n",
    "Record your thoughts here.\n",
    "\n",
    "They agree most often when both show no opioid use (self-report = 0 and hair tox = 0).\n",
    "\n",
    "They also agree when both show use (self-report = 1 and hair tox = 1), though this is less common.\n",
    "\n",
    "The main disagreement is when self-report = 0 but hair tox = 1, meaning someone said ‚Äúno use‚Äù but tested positive.\n",
    "\n",
    "This disagreement happens in 1.18% of cases.\n",
    "\n",
    "There are almost no cases where someone reported use but the toxicology was negative.\n",
    "\n",
    "/\n",
    "\n",
    "The 1.18% mismatch (self-report = 0, hair tox = 1) suggests under-reporting of opioid use in this sample.\n",
    "\n",
    "This means some participants may deny or fail to report use, but the biological test detects it.\n",
    "\n",
    "There is little to no evidence of over-reporting, since only a few people say they used opioids when the test does not confirm it.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Helper: Reformat cross-tabs with plainer language\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLEARER VIEW: Self-Report vs Hair Toxicology Agreement\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nüìä RAW COUNTS (how many people fall into each combination):\")\n",
    "print(contingency)\n",
    "\n",
    "print(\"\\n\\nüìà ROW PERCENTAGES (within each self-report group, what % had each hair result):\")\n",
    "row_pct = (contingency.div(contingency.sum(axis=1), axis=0) * 100).round(1)\n",
    "print(row_pct)\n",
    "\n",
    "print(\"\\n\\nüîç KEY INSIGHT:\")\n",
    "print(f\"   Among people who reported NO opioid use:\")\n",
    "print(f\"   ‚Üí {discordant_percent:.1f}% had a POSITIVE hair test\")\n",
    "print(f\"   ‚Üí This suggests possible under-reporting or recent use not disclosed\")\n",
    "\n",
    "print(\"\\n‚úÖ Agreement rates:\")\n",
    "agree_both_no = contingency.loc[0, 0] if 0 in contingency.index and 0 in contingency.columns else 0\n",
    "agree_both_yes = contingency.loc[1, 1] if 1 in contingency.index and 1 in contingency.columns else 0\n",
    "total_pairs = contingency.sum().sum()\n",
    "agreement_pct = ((agree_both_no + agree_both_yes) / total_pairs * 100) if total_pairs > 0 else 0\n",
    "print(f\"   ‚Ä¢ Both report NO and test NEG: {agree_both_no} people\")\n",
    "print(f\"   ‚Ä¢ Both report YES and test POS: {agree_both_yes} people\")\n",
    "print(f\"   ‚Ä¢ Overall agreement: {agreement_pct:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### Visualizing the contingency table\n",
    "\n",
    "Tables are powerful, but visualizations can make patterns easier to see.  \n",
    "Here we‚Äôll add two complementary plots:\n",
    "\n",
    "1. **Heatmap of counts** ‚Äì Shows the raw number of cases in each cell of the contingency table.  \n",
    "   Darker colors indicate larger counts.\n",
    "2. **Stacked bar chart of row proportions** ‚Äì Shows the percentage breakdown of hair toxicology results *within* each self-report group.\n",
    "\n",
    "These plots give both a ‚Äúhow many‚Äù perspective (counts) and a ‚Äúhow likely‚Äù perspective (proportions).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Visualization 1: Heatmap of counts (2x2 + Missing as category) ---\n",
    "# Ensure a consistent order for categories\n",
    "cat_order = [0, 1, 'Missing']\n",
    "cont_viz = pd.crosstab(x.fillna('Missing'), y.fillna('Missing'))\\\n",
    "            .reindex(index=cat_order, columns=cat_order, fill_value=0)\n",
    "\n",
    "plt.figure(figsize=(5.5, 4.5))\n",
    "plt.imshow(cont_viz.values, aspect='auto')\n",
    "plt.title('Self-report (rows) vs Hair tox (cols): Counts')\n",
    "plt.xlabel(hair_screen)\n",
    "plt.ylabel(self_report)\n",
    "plt.xticks(ticks=range(len(cat_order)), labels=cat_order)\n",
    "plt.yticks(ticks=range(len(cat_order)), labels=cat_order)\n",
    "cbar = plt.colorbar()\n",
    "cbar.set_label('Count')\n",
    "\n",
    "# Annotate cells with values\n",
    "for i in range(cont_viz.shape[0]):\n",
    "    for j in range(cont_viz.shape[1]):\n",
    "        plt.text(j, i, int(cont_viz.values[i, j]),\n",
    "                 ha='center', va='center', color='white', fontsize=10)\n",
    "plt.tight_layout()\n",
    "savefig(\"q5_heatmap_counts\")\n",
    "plt.show()\n",
    "\n",
    "# --- Visualization 2: Stacked bar of row proportions ---\n",
    "row_props_viz = cont_viz.div(cont_viz.sum(axis=1), axis=0).fillna(0)\n",
    "\n",
    "plt.figure(figsize=(6.5, 4.5))\n",
    "bottom = np.zeros(row_props_viz.shape[0])\n",
    "for col in row_props_viz.columns:\n",
    "    plt.bar(row_props_viz.index.astype(str), row_props_viz[col].values, bottom=bottom, label=str(col))\n",
    "    bottom += row_props_viz[col].values\n",
    "\n",
    "plt.title('Row proportions: Hair tox within Self-report groups')\n",
    "plt.xlabel(f'{self_report} (row groups)')\n",
    "plt.ylabel('Proportion within row')\n",
    "plt.legend(title=f'{hair_screen}', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "plt.ylim(0, 1)\n",
    "plt.tight_layout()\n",
    "savefig(\"q5_row_props\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "### Interpreting the visualizations\n",
    "\n",
    "Recall:  \n",
    "- **Self-report variable (`su_y_sui__use__rxopi_001`)** ‚Äì Past 6-month opioid use (1 = yes, 0 = no, NaN = missing).  \n",
    "- **Hair toxicology variable (`su_y_hairtox__rslt__opi_scrn`)** ‚Äì Positive hair screen for opioids (1 = positive, 0 = negative, NaN = missing).\n",
    "\n",
    "**Heatmap of counts**  \n",
    "- Larger numbers along the diagonal (0‚Äì0, 1‚Äì1) suggest agreement between self-report and hair toxicology.  \n",
    "- Off-diagonal cells (0‚Äì1, 1‚Äì0) represent **discordance** ‚Äî where self-report and hair tox give different answers.\n",
    "\n",
    "**Stacked bar chart of row proportions**  \n",
    "- Each bar represents a self-report group (0 = reported no use, 1 = reported use).  \n",
    "- The colored segments show the proportion of positive vs. negative hair tox results in each group.  \n",
    "- This proportional view is especially useful when the number of people in each self-report group is very different.\n",
    "\n",
    "**Tip:** Ask copilot to help you organize the cross-tabs into a clearer format with plainer language, this will help you read and understand the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "\n",
    "**Reflection:**\n",
    "\n",
    "Describe any patterns you observe in the contingency table and proportions.  Do the variables appear to be associated?  Are there particular cells where the counts are unexpectedly high or low?\n",
    "- \n",
    "\n",
    "Most people say they did not use opioids, and many of them also have missing hair tox results.\n",
    "\n",
    "People who report using opioids are much more likely to test positive on the hair tox test.\n",
    "\n",
    "A small number of people say ‚Äúno use‚Äù but test positive, which suggests some under-reporting.\n",
    "\n",
    "Almost no one says ‚Äúyes‚Äù but tests negative.\n",
    "\n",
    "Yes, the two variables appear to be associated because the hair tox results change a lot depending on what people report.\n",
    "\n",
    "The ‚Äúno use + missing tox‚Äù cell is very large, and the ‚Äúyes use + negative tox‚Äù cell is very small."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "## 6. Your Turn: Categorical √ó Numeric (Qualitative Variable √ó Quantitative Variable)\n",
    "\n",
    "In this activity, you‚Äôll examine how a **numeric** outcome varies across groups of a **categorical** predictor using **side-by-side boxplots**. This is useful for spotting group differences, spread, and outliers.\n",
    "\n",
    "**Steps you‚Äôll do:**\n",
    "1. Pick **one categorical** variable and **one numeric** variable from the lists below.\n",
    "2. Treat nonresponse codes (e.g., `999`) as a separate category for EDA (you‚Äôll clean in Lab 3).\n",
    "3. Create a grouped summary table (mean/median/n).\n",
    "4. Plot **side-by-side boxplots**.\n",
    "5. Jot quick interpretation notes.\n",
    "\n",
    "**Qual (categorical) options (pick one):**\n",
    "- `su_y_sui__use__rxopi_001` ‚Äì Ever used opioids  \n",
    "- `su_y_perc__rxopi_001` ‚Äì Perceived harm of use  \n",
    "- `su_y_otu__rxopi_001` ‚Äì Had opportunity to use opioid (1=yes, 0=no)  \n",
    "- `su_y_ptu__rxopi_001` ‚Äì Friend acceptance of opioid use (0, 1, 2; 999=missing)  \n",
    "\n",
    "**Quant (numeric) options (pick one):**\n",
    "- `su_y_drgprob_prsum` ‚Äì DAPI sum score  \n",
    "- `su_y_sui__rxopi__onset_useage` ‚Äì Age at first opioid use  \n",
    "- `su_y_sui__rxopi__lt_001` ‚Äì Number of opioid use days (past 6 months)  \n",
    "- `fc_y_pm_mean` ‚Äì Parental monitoring (mean)\n",
    "\n",
    "> ‚ö†Ô∏è Reminder: For this lab, keep `999` as a **category** (‚ÄúMissing‚Äù) for the **categorical** variable. For the **numeric** variable, keep it numeric and drop NaNs.\n",
    "\n",
    "---\n",
    "\n",
    "### ü§ñ Copilot prompt (paste into a comment)\n",
    "‚ÄúShow pandas code to compare a categorical and numeric variable:  \n",
    "- map `999` to a 'Missing' level for the categorical,  \n",
    "- compute groupwise n/mean/median,  \n",
    "- and draw side-by-side boxplots (numeric by category) with clear axis labels.‚Äù\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== Your Turn: Qual √ó Quant  ======\n",
    "\n",
    "# 1) Choose one categorical and one numeric variable from the lists above\n",
    "cat_var = 'su_y_sui__use__rxopi_001'    # e.g., 'su_y_ptu__rxopi_001'\n",
    "num_var = 'su_y_sui__rxopi__onset_useage'    # e.g., 'su_y_drgprob_prsum'\n",
    "\n",
    "# 2) Configure missing/nonresponse for the categorical variable\n",
    "MISSING_CODE_CAT = 999        # e.g., 999\n",
    "MISSING_LABEL    = 'Missing'      # e.g., 'Missing'\n",
    "\n",
    "# 3) Prepare series\n",
    "cat_series = df[cat_var].replace(MISSING_CODE_CAT, MISSING_LABEL).fillna('Missing')  # same label as above\n",
    "num_series = pd.to_numeric(df[num_var], errors='coerce')  # keep numeric; coerce non-numeric to NaN\n",
    "\n",
    "# 4) Combine into a working DataFrame\n",
    "work = pd.DataFrame({\n",
    "    'cat': cat_series.astype('category'),\n",
    "    'num': num_series\n",
    "})\n",
    "\n",
    "# 5) (Optional) drop rows where numeric is NaN before summaries/plots\n",
    "drop_na_numeric = True   # True / False\n",
    "if drop_na_numeric:\n",
    "    work = work.dropna(subset=['num'])\n",
    "\n",
    "# 6) Grouped summary (n, mean, median)\n",
    "grouped = work.groupby('cat')['num'].agg(n='count', mean='mean', median='median').reset_index()\n",
    "\n",
    "print(\"Grouped summary (numeric by category):\")\n",
    "display(grouped)\n",
    "\n",
    "# 7) Side-by-side boxplots (numeric by category)\n",
    "fig_w, fig_h = 7, 4   # e.g., 7, 4\n",
    "\n",
    "ax = work.boxplot(\n",
    "    column='num',\n",
    "    by='cat',\n",
    "    figsize=(fig_w, fig_h),\n",
    "    grid=False                 # True/False\n",
    ")\n",
    "ax.set_title(f\"{num_var} by {cat_var} (Boxplots)\")\n",
    "ax.set_xlabel(cat_var)\n",
    "ax.set_ylabel(num_var)\n",
    "plt.suptitle(\"\")            # remove pandas‚Äô automatic super title\n",
    "plt.xticks(rotation=0)    # e.g., 0 or 45\n",
    "plt.tight_layout()\n",
    "savefig(f\"q6_box_{num_var}_by_{cat_var}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "### Interpretation Helper\n",
    "\n",
    "Use the following prompts to guide your interpretation of the boxplots:\n",
    "\n",
    "- **Plain language:** What does `cat_var` represent, and what do its codes mean?  \n",
    "  *(e.g., 0 = No, 1 = Yes, 2 = Higher acceptance, 999/Missing = nonresponse)*  \n",
    "- **Plain language:** What does `num_var` measure (units or scale)?\n",
    "- **Boxplots:** Compare medians, IQR (box height), and outliers across categories.\n",
    "- Do certain categories have consistently higher or lower medians?\n",
    "- Does variability (spread) differ across categories?\n",
    "- Any potential data quality notes?  \n",
    "  *(e.g., a \"Missing\" category with unusual distribution)*\n",
    "- **Takeaway:** Write one sentence about the possible association you observe.  \n",
    "  *(Exploratory only; no formal inference at this stage.)*\n",
    "\n",
    "**Tip:** Ask copilot to help you organize the output and labels on the visualizations into a clearer format with plainer language, this will help you read and understand the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## 7. Bivariate EDA: Quantitative √ó Quantitative (Demo)\n",
    "\n",
    "Finally we'll explore a relationship between two numeric variables.  We'll create a **peer deviance sum** by adding three Likert‚Äëscaled items (`fc_y_pbp_001`, `fc_y_pbp_002`, and `fc_y_pbp_003`).  Before summing, we replace any special missing codes (777 and 999) with `NaN`.  We then plot the resulting sum against the DAPI sum and compute the **Spearman correlation coefficient**.  Spearman's rho is appropriate here because both variables are discrete and may not be normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Replace special codes with NaN for the peer deviance items\n",
    "peer_items = df[['fc_y_pbp_001', 'fc_y_pbp_002', 'fc_y_pbp_003']].replace({777: np.nan, 999: np.nan})\n",
    "\n",
    "# Compute peer deviance sum\n",
    "peer_dev_sum = peer_items.sum(axis=1)\n",
    "\n",
    "# Extract DAPI sum\n",
    "dapi = df['su_y_drgprob_prsum']\n",
    "\n",
    "# Scatter plot\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(peer_dev_sum, dapi, alpha=0.5)\n",
    "plt.title('Peer deviance sum vs. DAPI sum')\n",
    "plt.xlabel('Peer deviance sum (sum of 3 items)')\n",
    "plt.ylabel('DAPI sum')\n",
    "savefig(\"q7_scatter_peer_vs_dapi\")\n",
    "plt.show()\n",
    "\n",
    "# Spearman correlation (drop missing pairs)\n",
    "mask = (~peer_dev_sum.isna()) & (~dapi.isna())\n",
    "if mask.sum() > 1:\n",
    "    rho, pval = spearmanr(peer_dev_sum[mask], dapi[mask])\n",
    "    print(f\"Spearman correlation: {rho:.3f}\")\n",
    "else:\n",
    "    print(\"Not enough data to compute Spearman correlation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "\n",
    "**Reflection:**\n",
    "\n",
    "- Is there an apparent relationship between the peer deviance sum and the DAPI sum?  Does the scatter plot suggest a positive, negative, or no association?\n",
    "- How strong is the Spearman correlation coefficient?  Would you consider this a weak, moderate, or strong relationship?\n",
    "\n",
    "Comment on your observations here.\n",
    "\n",
    "There is a slight positive relationship between peer deviance and the DAPI sum, because higher peer deviance scores tend to show slightly higher DAPI scores in the scatter plot.\n",
    "\n",
    "\n",
    "The Spearman correlation is 0.272, which indicates a weak to moderate positive relationship.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Tip:** Ask copilot to help you organize the output and labels on the visualizations into a clearer format with plainer language, this will help you read and understand the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "## 8. Your Turn: Quantitative √ó Quantitative (PGD Sum √ó Second Quantitative Variable)\n",
    "\n",
    "Now you will repeat the process from the demo, but:\n",
    "- Create a **Peer Group Deviance (PGD) sum** from `su_y_pgd_001` through `su_y_pgd_008`.\n",
    "- Replace special missing codes (`777`, `999`) with `NaN` before summing.\n",
    "- Pick **one** numeric variable from the list below to compare with your PGD sum:\n",
    "  - `su_y_drgprob_prsum` ‚Äì DAPI summary\n",
    "  - `su_y_sui__rxopi__onset_useage` ‚Äì Age at first opioid use\n",
    "  - `su_y_sui__rxopi__lt_001` ‚Äì Number of opioid use days (past 6 months)\n",
    "  - `fc_y_pm_mean` ‚Äì Parental monitoring mean\n",
    "- Create a scatter plot and compute **Spearman‚Äôs correlation**.\n",
    "\n",
    "---\n",
    "\n",
    "**ü§ñ Copilot prompt:**\n",
    "> ‚ÄúReplace special codes (777, 999) with NaN for 8 PGD items, sum them, plot against a chosen numeric variable with scatter plot, and compute Spearman correlation (pandas + matplotlib + scipy).‚Äù\n",
    "> ‚ÄúCheck my labels: rewrite them in plainer language so a non-technical reader understands them immediately.‚Äù\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "\n",
    "# 1) Replace special codes with NaN for the PGD items\n",
    "pgd_items = df[['su_y_pgd_001', 'su_y_pgd_002', 'su_y_pgd_003', 'su_y_pgd_004', \n",
    "                'su_y_pgd_005', 'su_y_pgd_006', 'su_y_pgd_007', 'su_y_pgd_008']] \\\n",
    "    .replace({777: np.nan, 999: np.nan})  # e.g., 777, 999\n",
    "\n",
    "# 2) Compute PGD sum\n",
    "pgd_sum = pgd_items.sum(axis=1)\n",
    "\n",
    "# 3) Extract the second numeric variable\n",
    "num2_var = 'su_y_drgprob_prsum'  # e.g., 'su_y_drgprob_prsum'\n",
    "num2 = df[num2_var]\n",
    "\n",
    "# 4) Scatter plot\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.scatter(pgd_sum, num2, alpha=0.5)  # e.g., 0.5\n",
    "plt.title(f\"PGD sum vs. {num2_var}\")\n",
    "plt.xlabel(\"PGD sum (sum of 8 items)\")\n",
    "plt.ylabel(num2_var)\n",
    "savefig(f\"q8_scatter_pgd_vs_{num2_var}\")\n",
    "plt.show()\n",
    "\n",
    "# 5) Spearman correlation (drop missing pairs)\n",
    "mask = (~pgd_sum.isna()) & (~num2.isna())\n",
    "if mask.sum() > 1:\n",
    "    rho, pval = spearmanr(pgd_sum[mask], num2[mask])\n",
    "    print(f\"Spearman correlation: {rho:.3f}\")\n",
    "else:\n",
    "    print(\"Not enough data to compute Spearman correlation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "### Interpretation Helper\n",
    "\n",
    "- **PGD sum meaning:** What does a high PGD score indicate? What‚Äôs the possible range?\n",
    "- **Second variable meaning:** What does your chosen numeric variable measure?\n",
    "- **Pattern:** Does the scatter plot suggest a positive, negative, or no association?\n",
    "- **Strength:** Is Spearman‚Äôs rho close to 0, moderate (0.3‚Äì0.5), or high (>0.5)?\n",
    "- **Missing data:** Were many pairs dropped? Could this affect your conclusions?\n",
    "- **One-sentence takeaway:** Summarize the observed association in plain language.\n",
    "\n",
    "**Tip:** Ask copilot to help you organize the output and labels on the visualizations into a clearer format with plainer language, this will help you read and understand the output.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "# Lab 2 ‚Äî Autograder Checklist & Grade Rubric *(Dataset-Free)* (**50 pts**)\n",
    "\n",
    "This autograder **does not execute your notebook or require any dataset**. It parses `Lab2.ipynb` and checks for **variable names, assignments, and code patterns**. Keep the **exact names** and show the required **expressions** in code cells.\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ What you must include (names & patterns must match)\n",
    "\n",
    "**You may choose any valid columns**, but your code must define the items below exactly as named.\n",
    "\n",
    "### Univariate ‚Äî Numeric\n",
    "- `quant_var` ‚Äî a **string** set to an approved numeric column name (shown in the notebook).\n",
    "- `series` ‚Äî created from `df[quant_var]` with special codes replaced:  \n",
    "  `series = df[quant_var].replace({777: np.nan, 999: np.nan})`\n",
    "- Scalars defined **from `series`**:  \n",
    "  `mean`, `median`, `q1`, `q3`, `iqr` (`q3 - q1`), `pct_missing`, `min_val`, `max_val`  \n",
    "  - `pct_missing` must appear as: `series.isna().mean() * 100`\n",
    "- Figure-save lines (filenames must match):  \n",
    "  `plt.savefig(f\"hist_{quant_var}.png\", ...)` and `plt.savefig(f\"box_{quant_var}.png\", ...)`\n",
    "\n",
    "### Univariate ‚Äî Categorical\n",
    "- `qual_var` ‚Äî a **string** set to an approved categorical column name.\n",
    "- `series` ‚Äî built from `df[qual_var]` (OK to reuse the name) and, if applicable, replace special codes (e.g., `{999: np.nan}`).\n",
    "- Counts & proportions **with required options**:\n",
    "  - `counts = series.value_counts(dropna=False)`\n",
    "  - `proportions = series.value_counts(normalize=True, dropna=False)`\n",
    "- `freq_table` ‚Äî a DataFrame with **columns** `count` and `proportion` created from `counts` and `proportions`.\n",
    "- Figure-save line:  \n",
    "  `plt.savefig(f\"bar_{qual_var}.png\", ...)`\n",
    "\n",
    "### Bivariate ‚Äî Names only (syntax check)\n",
    "- `cat_var` ‚Äî from the approved categorical list.\n",
    "- `num_var` ‚Äî from the approved quantitative list.\n",
    "- `num2_var` ‚Äî from the approved quantitative list.\n",
    "\n",
    "> You **do not upload images**. The autograder just verifies your code contains the correct `savefig(...)` calls and required variable definitions.\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Grade Rubric (how points are earned)\n",
    "\n",
    "### 1) Smoke & Structure (**5 pts**)\n",
    "- **2 pts** ‚Äî No unfinished placeholders: `____`, `TODO`, `pass  # TODO`.\n",
    "- **3 pts** ‚Äî Notebook shows the standard imports (e.g., `import pandas as pd`) **and** assigns a variable named `df` (e.g., a `pd.read_csv(...)` into `df`).\n",
    "\n",
    "### 2) Numeric Summary (**12.5 pts**)\n",
    "- **3 pts** ‚Äî `quant_var` set to an approved name **and** `series = df[quant_var].replace({777: np.nan, 999: np.nan})` appears.\n",
    "- **4 pts** ‚Äî `mean`, `median`, `min_val`, `max_val` are computed from `series` (e.g., `series.mean()`, `series.median()`, `series.min()`, `series.max()`).\n",
    "- **3 pts** ‚Äî `q1 = series.quantile(0.25)`, `q3 = series.quantile(0.75)`, and `iqr = q3 - q1`.\n",
    "- **2.5 pts** ‚Äî `pct_missing` appears exactly as `series.isna().mean() * 100`.\n",
    "\n",
    "### 3) Categorical Frequencies (**12.5 pts**)\n",
    "- **3 pts** ‚Äî `qual_var` set to an approved name; if needed, special-code replacement shown.\n",
    "- **6 pts** ‚Äî `counts = series.value_counts(dropna=False)` **and** `freq_table` includes a `count` column built from `counts`.\n",
    "- **3.5 pts** ‚Äî `proportions = series.value_counts(normalize=True, dropna=False)` **and** `freq_table` includes a `proportion` column built from `proportions`.\n",
    "\n",
    "### 4) Figure Save Calls (**10 pts**)\n",
    "- **3 pts** ‚Äî Code includes `plt.savefig(f\"hist_{quant_var}.png\", ...)`.\n",
    "- **3 pts** ‚Äî Code includes `plt.savefig(f\"box_{quant_var}.png\", ...)`.\n",
    "- **4 pts** ‚Äî Code includes `plt.savefig(f\"bar_{qual_var}.png\", ...)`.\n",
    "\n",
    "### 5) Bivariate Variable Selection (syntax only) (**10 pts**)\n",
    "- **3 pts** ‚Äî `cat_var` is from the approved categorical list.\n",
    "- **3 pts** ‚Äî `num_var` is from the approved quantitative list.\n",
    "- **4 pts** ‚Äî `num2_var` is from the approved quantitative list.\n",
    "\n",
    "**Total = 50 pts**\n",
    "\n",
    "---\n",
    "\n",
    "## ‚òëÔ∏è Quick Pre-Submission Checklist\n",
    "\n",
    "- [ ] I set `quant_var` and `qual_var` to approved column names.\n",
    "- [ ] I defined **all** required scalars from `series`: `mean`, `median`, `q1`, `q3`, `iqr`, `pct_missing`, `min_val`, `max_val`.\n",
    "- [ ] My `pct_missing` line is exactly `series.isna().mean() * 100`.\n",
    "- [ ] I created `counts`, `proportions` with `dropna=False` and built `freq_table` with `count` and `proportion`.\n",
    "- [ ] I included these save lines:  \n",
    "  `hist_{quant_var}.png`, `box_{quant_var}.png`, `bar_{qual_var}.png`.\n",
    "- [ ] I set `cat_var`, `num_var`, `num2_var` from the approved lists.\n",
    "- [ ] I removed all placeholders and kept required names.\n",
    "- [ ] The notebook opens without errors and contains the expected imports and `df = ...` assignment.\n",
    "\n",
    "---\n",
    "\n",
    "## üîé Common Pitfalls\n",
    "\n",
    "- Hiding errors with `try/except`.\n",
    "- Renaming or omitting required variables/filenames.\n",
    "- Missing `dropna=False` or `normalize=True` in `value_counts(...)`.\n",
    "- Not showing the exact `pct_missing` expression.\n",
    "- Using different `savefig` filenames or leaving out the calls.\n",
    "\n",
    "---\n",
    "\n",
    "## üì§ Submission\n",
    "\n",
    "Upload **only** `Lab2.ipynb` to Gradescope. The autograder reads your notebook and checks for the required names and code patterns‚Äî**no dataset needed**.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
